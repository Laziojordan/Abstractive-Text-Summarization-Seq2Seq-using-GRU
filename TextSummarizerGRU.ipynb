{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarizerGRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyhamAlomari/Abstractive-Text-Summarization-Seq2Seq-using-GRU/blob/master/TextSummarizerGRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04lktufD0Xh",
        "colab_type": "text"
      },
      "source": [
        "**References:**\n",
        "\n",
        "1- Sequence Models (Course 5 of the Deep Learning Specialization), Deeplearning.ai, **Andrew Ng**\n",
        "\n",
        "https://www.youtube.com/playlist?list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6\n",
        "\n",
        "2-\tComprehensive Guide to Text Summarization using Deep Learning in Python, **Aravind Pai**\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/#comment-160299\n",
        "\n",
        "3- A ten-minute introduction to sequence-to-sequence learning in Keras, **Francois Chollet**\n",
        "\n",
        "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w9UzOpD9jMu",
        "colab_type": "text"
      },
      "source": [
        "**Abstractive Text Summarization**\n",
        "\n",
        "First, we can form our problem as a sequence-to-sequence problem (a sequence prediction problem) at word-level where the input sequences are long english statement(s) and the output is a shorter-length, same-meaning, different-words summary. \n",
        "\n",
        "In general, the input sequences and output sequences in **sequence-to-sequence learning** have different lengths (such as text summarization and machine translation) and the entire input sequence is required in order to start predicting the target.\n",
        "\n",
        "In Keras, the process is done as follow:\n",
        "- A RNN layer (Here, we will use GRU) acts as \"Encoder\": it processes the input sequence and returns its own internal state. \n",
        "- Another RNN layer (GRU) acts as \"Decoder\": it is trained to predict the next words of the target sequence, given previous words of the target sequence.\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "Amazon Fine Food reviews\n",
        "\n",
        "**Other Required Files**\n",
        "\n",
        "attention layer file.\n",
        "\n",
        "Note: Keras has a built-in Attention class. Unfortunately, it is more suitable for CNNs than RNNs. Therefore, we will use a third-paty attention layer file.\n",
        "\n",
        "Now, Let's start coding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCpvcPsdQ_Kt",
        "colab_type": "text"
      },
      "source": [
        "The code will follow these steps:\n",
        "\n",
        "1- Importing Libraries and Reading the dataset\n",
        "\n",
        "2- Dataset general information\n",
        "\n",
        "3- Dropping unuseful columns and null/duplicated rows\n",
        "\n",
        "4- Dataset Preprocessing and Cleaning\n",
        "\n",
        "5- Splitting the dataset into Training and Testing\n",
        "\n",
        "6- Determining the maximum lengths of texts and summaries\n",
        "\n",
        "7- Preparing our data for deep learning (Tokenizer/Vocabulary)\n",
        "\n",
        "8- Building the model\n",
        "\n",
        "A) Setting up the Model\n",
        "\n",
        "B) Training the Model\n",
        "\n",
        "9) Inference Model (Predicting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJw9nGZ2jAUr",
        "colab_type": "text"
      },
      "source": [
        "#**1- Importing Libraries and Reading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAWfWNKzgMmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow.keras\n",
        "#import tensorflow as tf\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, Embedding, TimeDistributed, Concatenate\n",
        "#from tensorflow.keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#from tensorflow.keras.layers import Attention\n",
        "from attention import AttentionLayer\n",
        "from sklearn import model_selection\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "\n",
        "dataset=pd.DataFrame(pd.read_csv(\"Reviews.csv\",nrows=100000))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7yhfvkvoO8D",
        "colab_type": "text"
      },
      "source": [
        "#  **2- Dataset general information**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bc1GvzapcbW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "1.   shape\n",
        "2.   columns names\n",
        "3.   null rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ylhumq4qFIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a62230e7-0bd0-487b-893e-f36f08e735f5"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpqMDbJpqA1i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d68aa71e-aec3-45e2-df8a-c37f71893bc3"
      },
      "source": [
        "dataset.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
              "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZU4NggXf8bV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "e07f69ec-5972-4642-9ea1-561398b14246"
      },
      "source": [
        "dataset.info()\n",
        "dataset.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count   Dtype \n",
            "---  ------                  --------------   ----- \n",
            " 0   Id                      100000 non-null  int64 \n",
            " 1   ProductId               100000 non-null  object\n",
            " 2   UserId                  100000 non-null  object\n",
            " 3   ProfileName             99996 non-null   object\n",
            " 4   HelpfulnessNumerator    100000 non-null  int64 \n",
            " 5   HelpfulnessDenominator  100000 non-null  int64 \n",
            " 6   Score                   100000 non-null  int64 \n",
            " 7   Time                    100000 non-null  int64 \n",
            " 8   Summary                 99998 non-null   object\n",
            " 9   Text                    100000 non-null  object\n",
            "dtypes: int64(5), object(5)\n",
            "memory usage: 7.6+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                        0\n",
              "ProductId                 0\n",
              "UserId                    0\n",
              "ProfileName               4\n",
              "HelpfulnessNumerator      0\n",
              "HelpfulnessDenominator    0\n",
              "Score                     0\n",
              "Time                      0\n",
              "Summary                   2\n",
              "Text                      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReTwGaF3wH9D",
        "colab_type": "text"
      },
      "source": [
        "# **3- Dropping unuseful columns and null/duplicated rows**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL6sDodVvSLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dataset=dataset.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
        "                      'HelpfulnessDenominator', 'Score', 'Time'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NQ6IyRzvsVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bad804d5-8a90-47d6-947f-a94310b0f274"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAl_vsocvwEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "45fa9d43-36a4-4845-fb65-ab006b80d648"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count   Dtype \n",
            "---  ------   --------------   ----- \n",
            " 0   Summary  99998 non-null   object\n",
            " 1   Text     100000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSQz7FFhn2jM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "c9d47e0f-f226-4829-c4be-3e9f81c6e88a"
      },
      "source": [
        "dataset.dropna(inplace=True,axis=0)\n",
        "dataset.drop_duplicates(subset='Text')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>yummy!</td>\n",
              "      <td>I just love it and will buy another box when I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>Tastes like More!</td>\n",
              "      <td>My late father in law used to have a rating sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>Great ramen</td>\n",
              "      <td>This is my favorite brand of Korean ramen. It ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>Spicy!!</td>\n",
              "      <td>I do like these noodles although, to say they ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>This spicy noodle cures my cold, upset stomach...</td>\n",
              "      <td>I love this noodle and have it once or twice a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>88425 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Summary                                               Text\n",
              "0                                  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
              "1                                      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2                                  \"Delight\" says it all  This is a confection that has been around a fe...\n",
              "3                                         Cough Medicine  If you are looking for the secret ingredient i...\n",
              "4                                            Great taffy  Great taffy at a great price.  There was a wid...\n",
              "...                                                  ...                                                ...\n",
              "99995                                             yummy!  I just love it and will buy another box when I...\n",
              "99996                                  Tastes like More!  My late father in law used to have a rating sy...\n",
              "99997                                        Great ramen  This is my favorite brand of Korean ramen. It ...\n",
              "99998                                            Spicy!!  I do like these noodles although, to say they ...\n",
              "99999  This spicy noodle cures my cold, upset stomach...  I love this noodle and have it once or twice a...\n",
              "\n",
              "[88425 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OUMsqCpyXm4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a4e574ac-2696-4dd1-902b-2264af0f88ba"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Summary                                               Text\n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
              "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
              "4            Great taffy  Great taffy at a great price.  There was a wid..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLQfMbtzttYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "fb9875e4-9298-4147-9df7-1965fda5fd19"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 99998 entries, 0 to 99999\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   Summary  99998 non-null  object\n",
            " 1   Text     99998 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 2.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QttefTs8wvx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d2de109c-fab8-4dc0-955c-c4b762fb301e"
      },
      "source": [
        "dataset.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Summary    0\n",
              "Text       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEt5DidcyldL",
        "colab_type": "text"
      },
      "source": [
        "# **4- Dataset Preprocessing and Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxM1eYQvzAJi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Identifying contractions as a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIZvLYNczqxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n",
        "\"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\", \"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how does\",\n",
        "\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n",
        "\"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\",\n",
        "\"that'd\": \"that had\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "\"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "\"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n",
        "\"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrjGBcdsBg0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5pzaB5RBEUA",
        "colab_type": "text"
      },
      "source": [
        "2-   Converting all words in text and summary into lower case\n\n",
        "3-   Mapping contractions\n\n",
        "4-   Removing stopwords (except negation and comparing words; I think these words are important and affect the meaning of the whole sentence; so I kept them there)\n\n",
        "5-   Removing all special characters\n\n",
        "6-   Adding \"START\" and \"EOS\" tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZtxM6g96EdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Defining and customizing the stopwords \n",
        "all_stop_words=set(stopwords.words('english'))\n",
        "wanted_words=['so','not','no','nor','but','only','until','against','off','out','each','all','any','few','more','most',\n",
        "                'some','other','same','too','very','just', 'as','than']\n",
        "stop_words=[sw for sw in all_stop_words if not sw in wanted_words]\n",
        "\n",
        "#Defining the function of cleaning\n",
        "def cleaning_data(textdata):\n",
        "  #2\n",
        "   textdatanew = textdata.lower()\n",
        "  #3\n",
        "   textdatanew=[contractions[token] if token in contractions else token for token in textdatanew.split()]\n",
        "   textdatanew=\" \".join(textdatanew)\n",
        "  #Now, all negation expressions use the word \"not\"\n",
        "  #4\n",
        "   textdatanew=[s for s in textdatanew.split() if not s in stop_words]\n",
        "   textdatanew=\" \".join(textdatanew)\n",
        "  #5\n",
        "   textdatanew = re.sub(r'[^a-zA-Z]',' ', textdatanew)\n",
        "  \n",
        "   return textdatanew\n",
        "\n",
        "#modifying our dataset with new cleaned data\n",
        "new_text = []\n",
        "new_summary=[]\n",
        "for text in dataset.Text:\n",
        "    new_text.append(cleaning_data(text))\n",
        "    \n",
        "for summary in dataset.Summary:\n",
        "    new_summary.append(cleaning_data(summary))\n",
        "\n",
        "dataset['New_Text']=new_text\n",
        "dataset['New_Summary']=new_summary\n",
        "#6\n",
        "#dataset['New_Text'] = dataset['New_Text'].apply(lambda x : '<START> '+ x + ' <END>')\n",
        "dataset['New_Summary'] = dataset['New_Summary'].apply(lambda x : '<START>'+ x + '<EOS>')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fdTEdOriBhS",
        "colab_type": "text"
      },
      "source": [
        "Previewing the cleaned texts and summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4Eic-9ajI5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "a33f6d93-b17f-475d-b9f7-94734139c72f"
      },
      "source": [
        "#dataset.head()\n",
        "for i in range(7):\n",
        "  print(\"Text \",i+1,\": \")\n",
        "  print(dataset['New_Text'][i])\n",
        "  print(\"Summary: \")\n",
        "  print(dataset['New_Summary'][i])\n",
        "  print('-------------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text  1 : \n",
            "bought several vitality canned dog food products found all good quality  product looks more like stew than processed meat smells better  labrador finicky appreciates product better than most \n",
            "Summary: \n",
            "<START>good quality dog food<EOS>\n",
            "-------------------------------------\n",
            "Text  2 : \n",
            "product arrived labeled as jumbo salted peanuts   the peanuts actually small sized unsalted  not sure error vendor intended represent product as  jumbo  \n",
            "Summary: \n",
            "<START>not as advertised<EOS>\n",
            "-------------------------------------\n",
            "Text  3 : \n",
            "confection around few centuries  light  pillowy citrus gelatin nuts   case filberts  cut tiny squares liberally coated powdered sugar  tiny mouthful heaven  not too chewy  very flavorful  highly recommend yummy treat  familiar story c s  lewis   the lion  witch  wardrobe    treat seduces edmund selling out brother sisters witch \n",
            "Summary: \n",
            "<START> delight  says all<EOS>\n",
            "-------------------------------------\n",
            "Text  4 : \n",
            "looking secret ingredient robitussin believe found it  got addition root beer extract ordered  which good  made some cherry soda  flavor very medicinal \n",
            "Summary: \n",
            "<START>cough medicine<EOS>\n",
            "-------------------------------------\n",
            "Text  5 : \n",
            "great taffy great price  wide assortment yummy taffy  delivery very quick  taffy lover  deal \n",
            "Summary: \n",
            "<START>great taffy<EOS>\n",
            "-------------------------------------\n",
            "Text  6 : \n",
            "got wild hair taffy ordered five pound bag  taffy all very enjoyable many flavors  watermelon  root beer  melon  peppermint  grape  etc  only complaint bit too much red black licorice flavored pieces  just not particular favorites   me  kids  husband  lasted only two weeks  would recommend brand taffy    delightful treat \n",
            "Summary: \n",
            "<START>nice taffy<EOS>\n",
            "-------------------------------------\n",
            "Text  7 : \n",
            "saltwater taffy great flavors very soft chewy  each candy individually wrapped well  none candies stuck together  happen expensive version  fralinger s  would highly recommend candy  served beach themed party everyone loved it \n",
            "Summary: \n",
            "<START>great  just as good as expensive brands <EOS>\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDI-NImihdUA",
        "colab_type": "text"
      },
      "source": [
        "# **5- Splitting the dataset into Training and Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep05bq8whXkW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28f99a41-7624-4434-92b6-609fd509f964"
      },
      "source": [
        "print(dataset.shape)\n",
        "x_train, x_test, y_train, y_test= model_selection.train_test_split(dataset['New_Text'], dataset['New_Summary'], test_size=0.20,random_state=30, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(99998, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v_LQz-oGH2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv_0MW3ZCz_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3650bc1b-f32d-452c-d58b-8b8e65b30d1e"
      },
      "source": [
        "print('x_train: ',x_train.shape)\n",
        "print('y_train: ', y_train.shape)\n",
        "print('x_test: ', x_test.shape)\n",
        "print('y_test: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train:  (79998,)\n",
            "y_train:  (79998,)\n",
            "x_test:  (20000,)\n",
            "y_test:  (20000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxtISvYeh6_N",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **6- Determining the maximum lengths of texts and summaries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L03cMUAfQ00a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "85c11c15-e2db-4d8e-ee06-2cbb38204542"
      },
      "source": [
        "\n",
        "\n",
        "# def data_lengths(textdata):\n",
        "#   data_length=[]\n",
        "#   for example in textdata:\n",
        "#     data_length.append(len(example.split()))\n",
        "\n",
        "text_lengths=[]\n",
        "summary_lengths=[]\n",
        "\n",
        "for text in dataset['New_Text']:\n",
        "  text_lengths.append(len(text.split()))\n",
        "for summary in dataset.New_Summary:\n",
        "  summary_lengths.append(len(summary.split()))\n",
        "#text_lengths=data_lengths(dataset['Text'])\n",
        "#summary_lengths=data_lengths(dataset['Summary'])\n",
        "\n",
        "print(text_lengths[:10])\n",
        "print(summary_lengths[:10])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[28, 22, 47, 22, 14, 48, 32, 15, 17, 16]\n",
            "[4, 3, 4, 2, 2, 2, 8, 3, 2, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfCeM7bD7tsg",
        "colab_type": "text"
      },
      "source": [
        "Looking for the most suitable **length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCzSfCf7szH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "f0b81af7-4ffb-49bd-ccb4-f624f5c471e0"
      },
      "source": [
        "print('Text Lengths:')\n",
        "print(pd.DataFrame(text_lengths).describe())\n",
        "\n",
        "print(\"85% of text lengths is: \",np.percentile(text_lengths,85))\n",
        "print(\"90% of text lengths is: \",np.percentile(text_lengths,90))\n",
        "\n",
        "print('_____________________________')\n",
        "\n",
        "print('Summary Lengths:')\n",
        "print(pd.DataFrame(summary_lengths).describe())\n",
        "\n",
        "print(\"85% of summary lengths is: \",np.percentile(summary_lengths,85))\n",
        "print(\"90% of summary lengths is: \",np.percentile(summary_lengths,90))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text Lengths:\n",
            "                  0\n",
            "count  99998.000000\n",
            "mean      50.019490\n",
            "std       51.039005\n",
            "min        4.000000\n",
            "25%       21.000000\n",
            "50%       34.000000\n",
            "75%       60.000000\n",
            "max     1898.000000\n",
            "85% of text lengths is:  82.0\n",
            "90% of text lengths is:  101.0\n",
            "_____________________________\n",
            "Summary Lengths:\n",
            "                  0\n",
            "count  99998.000000\n",
            "mean       3.635013\n",
            "std        1.932578\n",
            "min        1.000000\n",
            "25%        2.000000\n",
            "50%        3.000000\n",
            "75%        5.000000\n",
            "max       22.000000\n",
            "85% of summary lengths is:  5.0\n",
            "90% of summary lengths is:  6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KJBNMM_AzOs",
        "colab_type": "text"
      },
      "source": [
        "Specifying the maximum lengths for both Texts and Summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m9tToxEAyUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_text_length=95\n",
        "max_summary_length=8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8I3MQcrGyb8",
        "colab_type": "text"
      },
      "source": [
        "# *7- Preparing our data for deep learning (Tokenizer/Vocabulary)*\n",
        "\n",
        "1- Building the vocabulary / Constructing the Tokenizer\n",
        "\n",
        "2- Assigning an integer \"index\" for each word, based on its frequencey on the \"fitting\" text; the first words are the most appeared\n",
        "\n",
        "3- Converting/Encoding each text/summary into its corresponding integer list/sequence based on the vocabulary built in steps 1 and 2\n",
        "\n",
        "4- padding each sequuence to the same length  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL24GWGBGm2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1\n",
        "text_tokenizer= Tokenizer()\n",
        "#2\n",
        "#fit the tokenizer on our text training data \"Filling the vocabulary\"\n",
        "text_tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "#some information about our vocabulary:\n",
        "#print(text_tokenizer.word_index)\n",
        "#print(text_tokenizer.word_counts)\n",
        "text_vocab_size=len(text_tokenizer.word_counts)+1\n",
        "print('Text Vocabulary Size= ',text_vocab_size)\n",
        "\n",
        "#print(text_tokenizer.document_count)\n",
        "#print(text_tokenizer.word_docs)\n",
        "\n",
        "#3\n",
        "x_train=text_tokenizer.texts_to_sequences(x_train)\n",
        "x_test=text_tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "#4\n",
        "x_train= pad_sequences(x_train, maxlen= max_text_length, padding='post', truncating='post', value=0)\n",
        "x_test= pad_sequences(x_test, maxlen= max_text_length, padding='post', truncating='post', value=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG9vmsDLd0qI",
        "colab_type": "text"
      },
      "source": [
        "The same goes to the summary side"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSvd5HyLOcxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1\n",
        "summary_tokenizer= Tokenizer()\n",
        "#2\n",
        "#fit the tokenizer on our text training data \"Filling the vocabulary\"\n",
        "summary_tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "#some information about our vocabulary:\n",
        "#print(summary_tokenizer.word_index)\n",
        "#print(summary_tokenizer.word_counts)\n",
        "summary_vocab_size=len(summary_tokenizer.word_counts)+1\n",
        "print('Summary Vocabulary Size= ',summary_vocab_size)\n",
        "\n",
        "#print(summary_tokenizer.document_count)\n",
        "#print(summary_tokenizer.word_docs)\n",
        "\n",
        "#3\n",
        "y_train=summary_tokenizer.texts_to_sequences(y_train)\n",
        "y_test=summary_tokenizer.texts_to_sequences(y_test)\n",
        "\n",
        "#4\n",
        "y_train= pad_sequences(y_train, maxlen= max_summary_length, padding='post', truncating='post', value=0)\n",
        "y_test= pad_sequences(y_test, maxlen= max_summary_length, padding='post', truncating='post', value=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZcsSupVgq_V",
        "colab_type": "text"
      },
      "source": [
        "# **8- Building the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlQHv0Yy6EQb",
        "colab_type": "text"
      },
      "source": [
        "Our model will be **encoder-decoder RNN**:\n",
        "\n",
        "**A) Setting up the Model:\n",
        "\n",
        "1- Defining the encoder\n",
        "\n",
        "2- Defining the decoder\n",
        "\n",
        "3- Using Attention Layer\n",
        "\n",
        "4- Defining the Dense Layer\n",
        "\n",
        "5- Defining the Model\n",
        "\n",
        "------------------------\n",
        "\n",
        "B) Training the Model:\n",
        "\n",
        "1- Compiling the model\n",
        "\n",
        "2- Defining the early stopping object\n",
        "\n",
        "3- Training/Fitting the model\n",
        "\n",
        "4- comparing training/validation progress\n",
        "\n",
        "-----------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuRsw8fVlZWG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# ****A) Setting up the Model:****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCOarLhUfYnS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "cf56bef4-ffa5-4398-f29b-bef37d7f27c6"
      },
      "source": [
        "backend.clear_session() \n",
        "features_units_num= 512\n",
        "\n",
        "#1\n",
        "enc_input= Input(shape=(max_text_length,))\n",
        "enc_embedding= Embedding(text_vocab_size, features_units_num, trainable= True)(enc_input)\n",
        "#Encoder Stacked GRU\n",
        "enc_gru1= GRU(features_units_num, return_state=True, return_sequences= True)\n",
        "enc_output1, h1= enc_gru1(enc_embedding)\n",
        "\n",
        "enc_gru2= GRU(features_units_num, return_sequences= True, return_state=True)\n",
        "enc_output2, h2= enc_gru2(enc_output1)\n",
        "\n",
        "enc_gru3= GRU(features_units_num, return_sequences= True, return_state=True)\n",
        "enc_output3, h3= enc_gru3(enc_output2)\n",
        "\n",
        "#2 \n",
        "dec_input= Input(shape=(None,))\n",
        "dec_embedding_layer= Embedding(summary_vocab_size, features_units_num, trainable= True)\n",
        "dec_emb=dec_embedding_layer(dec_input)\n",
        "\n",
        "#Decoder GRU\n",
        "dec_gru= GRU(features_units_num, return_sequences= True, return_state= True)\n",
        "dec_output, dec_h= dec_gru(dec_emb, initial_state=[h3])\n",
        "\n",
        "#3\n",
        "#Keras has a built-in Attention class. Unfortunately, it is more suitable for CNNs than RNNs.\n",
        "#Therefore, we will use a third-paty attention layer file.\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_output, attn_states = attn_layer([enc_output3, dec_output])\n",
        "input_layer = Concatenate(axis=-1, name='concat_layer')([dec_output, attn_output])\n",
        "\n",
        "#4\n",
        "#Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(summary_vocab_size, activation='softmax')) \n",
        "dec_output = decoder_dense(input_layer)\n",
        "\n",
        "#5\n",
        "# Define the model\n",
        "model = Model([enc_input, dec_input], dec_output) \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 95)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 95, 512)      24528896    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       [(None, 95, 512), (N 1575936     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     [(None, 95, 512), (N 1575936     gru[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 512)    6813184     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_2 (GRU)                     [(None, 95, 512), (N 1575936     gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_3 (GRU)                     [(None, None, 512),  1575936     embedding_1[0][0]                \n",
            "                                                                 gru_2[0][1]                      \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 512),  524800      gru_2[0][0]                      \n",
            "                                                                 gru_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 1024)   0           gru_3[0][0]                      \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 13307)  13639675    concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 51,810,299\n",
            "Trainable params: 51,810,299\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH133rlQgIjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #3\n",
        "# attention_output1= tensorflow.keras.layers.Attention()([enc_output3,dec_output])\n",
        "# # #dec_output1 = tensorflow.keras.layers.GlobalAveragePooling1D()(dec_output)\n",
        "# # #attention_output = tensorflow.keras.layers.GlobalAveragePooling1D()(attention_output1)\n",
        "# #input_layer = tensorflow.keras.layers.Concatenate()([dec_output1, attention_output])\n",
        "# input_layer = tensorflow.keras.layers.Concatenate()([dec_output, attention_output1])\n",
        "\n",
        "# # input_layer.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14eRIj_rmJf3",
        "colab_type": "text"
      },
      "source": [
        "# **B) Training the Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmHiJ5yLNV81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "#opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqVtD8sAOAgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2\n",
        "early_stop= EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Oz3D9qBOXwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "4b1c34a8-8398-45ea-f4ce-5481c9e71b6a"
      },
      "source": [
        "#3\n",
        "history=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:] ,epochs=15,callbacks=[early_stop],batch_size=128, validation_data=([x_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "625/625 [==============================] - 362s 580ms/step - loss: 3.2025 - val_loss: 2.8319\n",
            "Epoch 2/15\n",
            "625/625 [==============================] - 361s 577ms/step - loss: 2.7771 - val_loss: 2.6766\n",
            "Epoch 3/15\n",
            "625/625 [==============================] - 358s 572ms/step - loss: 2.5674 - val_loss: 2.6041\n",
            "Epoch 4/15\n",
            "625/625 [==============================] - 356s 570ms/step - loss: 2.3803 - val_loss: 2.5738\n",
            "Epoch 5/15\n",
            "625/625 [==============================] - 358s 572ms/step - loss: 2.1956 - val_loss: 2.5680\n",
            "Epoch 6/15\n",
            "625/625 [==============================] - 360s 577ms/step - loss: 2.0090 - val_loss: 2.5793\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E88BW-WVa7k9",
        "colab_type": "text"
      },
      "source": [
        "It is common to create dual learning curves for a machine learning model during training on both the training and validation datasets.\n",
        "\n",
        "Train Learning Curve: Learning curve calculated from the training dataset that gives an idea of how well the model is learning.\n",
        "\n",
        "Validation Learning Curve: Learning curve calculated from a hold-out validation dataset that gives an idea of how well the model is generalizing.\n",
        "\n",
        "In some cases, it is also common to create learning curves for multiple metrics, such as in the case of classification predictive modeling problems, where the model may be optimized according to cross-entropy loss and model performance is evaluated using classification accuracy. In this case, two plots are created, one for the learning curves of each metric, and each plot can show two learning curves, one for each of the train and validation datasets.\n",
        "\n",
        "Optimization Learning Curves: Learning curves calculated on the metric by which the parameters of the model are being optimized, e.g. loss.\n",
        "\n",
        "Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb3w0nCTNT5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5242b5ff-3bf8-46cf-8508-51bc07815c37"
      },
      "source": [
        "#4\n",
        "from matplotlib import pyplot \n",
        "pyplot.plot(history.history['loss'], label='train') \n",
        "pyplot.plot(history.history['val_loss'], label='test') \n",
        "pyplot.legend() \n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Yxi4znmYeR",
        "colab_type": "text"
      },
      "source": [
        "# **9) Inference Model (Predicting):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UVfdQ0kMVHk",
        "colab_type": "text"
      },
      "source": [
        "Now, it is time for predicting new sentences by the Inference model.\n",
        "First, we will build two standalone models, one for the encoder and the other for the decoder.\n",
        "\n",
        "- The Encoder Model is simple, it can be defined as taking the input layer from the trained model's encoder **enc_input**, and outputting the hidden and cell state tensors **enc_states**\n",
        "\n",
        "- The Decoder Model is more sophisticated, it takes the hidden and cell states from the new encoder model, **dec_input_state_h** as \"initial state\". But this state should be defined as an input first so it can be assigned to the decoder GRU layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsN3XugKqgEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Inference Encoder Model:\n",
        "encoder_inf_model=Model(enc_input, [enc_output3, h3])\n",
        "\n",
        "#Inference Decoder Model:\n",
        "dec_input_state_h= Input( shape=(features_units_num, ))\n",
        "dec_states_inputs=[dec_input_state_h]\n",
        "\n",
        "dec_hidden_state_input = Input(shape=(max_text_length, features_units_num ))\n",
        "dec_states_inputs_all=[dec_hidden_state_input, dec_input_state_h]\n",
        "\n",
        "dec_inf_embedding= dec_embedding_layer(dec_input)\n",
        "#dec_inf_embedding= Embedding(summary_vocab_size, features_units_num, trainable= True)(dec_input)\n",
        "\n",
        "dec_inf_output, state_h2 = dec_gru(dec_inf_embedding, initial_state=dec_states_inputs)\n",
        "\n",
        "#attention inference\n",
        "attn_inf_output, attn_inf_states = attn_layer([dec_hidden_state_input, dec_inf_output])\n",
        "inf_input_layer = Concatenate(axis=-1, name='concat')([dec_inf_output, attn_inf_output])\n",
        "\n",
        "dec_inf_output = decoder_dense(inf_input_layer)\n",
        "\n",
        "decoder_inf_model = Model( [dec_input] + dec_states_inputs_all, [dec_inf_output] + [state_h2])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx1rZ2kR15MR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    pred_output, pred_h = encoder_inf_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1)) #[[0]]\n",
        "\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = summary_tokenizer.word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h = decoder_inf_model.predict([target_seq] + [pred_output, pred_h])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if(sampled_token_index not in summary_tokenizer.index_word):\n",
        "          sampled_token='eos' #To avoid padding tokens\n",
        "        else:\n",
        "           sampled_token = summary_tokenizer.index_word[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='eos'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "            # Exit condition: either hit max length or find stop word.\n",
        "        if(sampled_token == 'eos' or len(decoded_sentence.split()) >= (max_summary_length-1)):\n",
        "                stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        pred_h = h\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx9ucHcp6HvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if((i!=0 and i!=summary_tokenizer.word_index['start']) and i!=summary_tokenizer.word_index['eos']):\n",
        "        newString=newString+summary_tokenizer.index_word[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if(i!=0):\n",
        "        newString=newString+text_tokenizer.index_word[i]+' '\n",
        "    return newString"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOjEE7hCr3Um",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67a7d5bc-1325-470b-ce70-72bf5eac061c"
      },
      "source": [
        "for i in range(230, 250):\n",
        "  print(\"Review:\",seq2text(x_test[i]))\n",
        "  print(\"Original summary:\",seq2summary(y_test[i]))\n",
        "  print(\"Predicted summary:\",decode_sequence(x_test[i].reshape(1,max_text_length)))\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: loved taste smell everything better than used make years ago br br and best delivery second party shipper ever received many thanks br br buy applesauce not disappointed \n",
            "Original summary: great taste good \n",
            "Predicted summary:  great\n",
            "\n",
            "\n",
            "Review: some best coffee out there as good not better than seattle s best starbuck good deal price wise order subscription do br smooth rich coffee prefer breakfast blend better than colombia blend changed to \n",
            "Original summary: community coffee \n",
            "Predicted summary:  excellent coffee\n",
            "\n",
            "\n",
            "Review: colombian ground coffee lovers brand really love wont sorry all purchasing \n",
            "Original summary: real tasty wonderful favor \n",
            "Predicted summary:  love it\n",
            "\n",
            "\n",
            "Review: really love brand coffee coffee nut enjoy drinking this nice smooth great tasting not go wrong choice \n",
            "Original summary: love brand coffee \n",
            "Predicted summary:  excellent coffee\n",
            "\n",
            "\n",
            "Review: community coffee colombia classico standard quality everyday type columbia coffee comparable eight clock columbia coffee good aroma first open bag one tablespoon per cup results decent strength coffee typical drip coffeemaker not premium coffee but becomes good value discount price \n",
            "Original summary: standard quality columbia coffee \n",
            "Predicted summary:  excellent coffee\n",
            "\n",
            "\n",
            "Review: community coffee best brand coffee made louisiana driving passed community coffee processing plant smelling coffee roasted experience one never forgets drank brand all adult life none compare it drank all varieties brand dark roast favorite until i tried columbia classico blend new favorite sweeter taste than other varieties not quiet as strong as dark roast but strong enough real coffee love so glad tried it thanks amazon carring especially subscribe save plan \n",
            "Original summary: love coffee \n",
            "Predicted summary:  best coffee ever\n",
            "\n",
            "\n",
            "Review: bought since seemed like great deal but made batch awful weak taste aroma as old coffee completely disappointing love trying all coffees expected more this never buy again thought would never say but dunkin doughnuts way superior so try first seattle best great coffee as well \n",
            "Original summary: poor taste \n",
            "Predicted summary:  not bad but not best\n",
            "\n",
            "\n",
            "Review: think best ground coffee bag do much better than store brands like maxwell house folgers wegmans etc sale actually same price would definitely try brand again br br this flavor strong bold side like \n",
            "Original summary: much better than store brand \n",
            "Predicted summary:  good coffee\n",
            "\n",
            "\n",
            "Review: good price community coffee pack received quickly product packages sealed but not firm bricks like expected say flavor not compromised that probably order again \n",
            "Original summary: good price community coffee \n",
            "Predicted summary:  good price\n",
            "\n",
            "\n",
            "Review: very good coffee definitely purchase again robust flavorful without any trace bitterness love it \n",
            "Original summary: delicioso \n",
            "Predicted summary:  very good\n",
            "\n",
            "\n",
            "Review: weak dry tasteless coffee not taste even remotely like made columbian arabica beans grind also very poor quality br br how bad it stuff makes folger s seem like premium high grade stuff no earthly idea people give community colombian classico high rating perhaps just got bad batch very old stuff amazon but i ll not giving coffee second chance probably one weakest most tasteless coffee beans i ve ever bought \n",
            "Original summary: horrible stuff \n",
            "Predicted summary:  horrible taste\n",
            "\n",
            "\n",
            "Review: not know name community coffee intended convey coffee masses but kinda would rate it smells good open package use normal amount brew good flavor aroma no disappointments but nothing special hard time distinguishing other typical brands columbian coffee no worse no better \n",
            "Original summary: good coffee but not special \n",
            "Predicted summary:  not like coffee\n",
            "\n",
            "\n",
            "Review: case half cans smashed i m not saying destroyed also within expiration date however foods some cans discolored must getting old br br so technically speaking nothing wrong them but made keep wondering safe so peace mind decided not order product again \n",
            "Original summary: could better \n",
            "Predicted summary:  dented cans\n",
            "\n",
            "\n",
            "Review: six cans smashed received maybe pack better shipment not send smashed cans \n",
            "Original summary: smashed cans \n",
            "Predicted summary:  dented cans\n",
            "\n",
            "\n",
            "Review: recently purchased item tried first cup last evening bit aftertaste sent searching ingredients list not individual cup but original packaging sucralose hiding ingredients which not big deal not taste enjoy prospective buyers aware presence artificial sweetener not advertised noted any way product description amazon the manufacturer website notes natural artificial sweeteners used certainly something i ll checking making future purchases type item overall flavor except sweetener issue acceptable but not look rich creamy body beverage \n",
            "Original summary: sweetened sucralose \n",
            "Predicted summary:  not buy product\n",
            "\n",
            "\n",
            "Review: taste chocolate almost made pack keurig take back store purchased hot cocoa out excitement brought mr coffee keurig sale black friday avid hot cocoa drinker far worst tasting chocolate opinion watery awful artificial sweetener aftertaste granted first time not shake k cup as directed so gave second try shaking surprisingly not d mn thing help taste bright side long watered artificial sweetener as taste drink k cup you \n",
            "Original summary: not flavor expecting hot cocoa \n",
            "Predicted summary:  yuck\n",
            "\n",
            "\n",
            "Review: recently tried milk chocolate k cup amazon only thing drink common chocolate color of items listed as ingredients cocoa number it chemical smell chemical taste if buy junk not say not warned not deserve any stars \n",
            "Original summary: chocolate lovers beware \n",
            "Predicted summary:  no more\n",
            "\n",
            "\n",
            "Review: love stuff make oz setting cuisinart k cup machine add frothed steamed milk frother makes wonderful hot chocolate latte chocolate very good itself as well although like add least splash milk or whipped cream thicken body little \n",
            "Original summary: expensive but tastes great \n",
            "Predicted summary:  great hot chocolate\n",
            "\n",
            "\n",
            "Review: excellent hot chocolate buy again br and recommend highly very good flavor very smooth br and actually tastes like chocolate \n",
            "Original summary: real winner \n",
            "Predicted summary:  excellent\n",
            "\n",
            "\n",
            "Review: family loves hot chocolate cold winter s day not hot chocolate not know is need find another product keurig looking hot chocolate please not waste money product no quality control one keurig thinks good beyond me \n",
            "Original summary: watered chocolate strong artificial sweetner taste \n",
            "Predicted summary:  best hot chocolate ever\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
